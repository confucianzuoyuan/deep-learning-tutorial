{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce2f954",
   "metadata": {},
   "source": [
    "# 准备数据集\n",
    "\n",
    "我们将《三国演义》的原文作为数据集, 来训练一个字符级别的语言模型. 也就是将原文中的汉字以及标点符号等等映射成整型(`int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a8541b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的包\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "51414500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符数据集的长度: 605,548\n"
     ]
    }
   ],
   "source": [
    "# 打开《三国演义》的文本文件`input.txt`\n",
    "# 然后读取\n",
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 打印《三国演义》中的字符数量\n",
    "print(f\"字符数据集的长度: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0040f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有不同的字符: \n",
      " <>[]—‘’“”…□　、。《》【】一丁七万丈三上下不与丐丑专且丕世丘丙业丛东丝丞丢两严丧个中丰临丸丹为主丽举乂乃久么义之乌乎乏乐乔乖乘乙九乞也习乡书买乱乳乾了予争事二于亏云互五井亘亚些亟亡亢交亥亦产亨亩享京亭亮亲亵亹人什仁仅仆仇今介仍从仓仔仕他仗付仙仞代令以仪们仰仲件价任仿伉伊伍伎伏伐休众优伙会伞伟传伤伦伪伯伴伷伸伺似但位低住佐佑体何佗余佛作佞你佣佥佩佯佳佻使侄侈例侍供依侠侥侧侪侮侯侵便促俄俊俎俗俘保俞俟信俦俨俭修俯俱俸俺俾倅倍倏倒倘候倚借倡倥倦值倾偃假偎偏偕做停健偬偶偷偿傅傍傕储催傲像僚僧僭僮僵僻儁儒儿兀允元兄充兆先光克免兔兖党兜兢入全八公六兮兰共关兴兵其具典兹养兼兽冀内冈册再冒冓冕冗写军农冠冢冤冥冬冯冰冲决况冶冷冻净凄准凉凋凌减凑凛凝几凡凤凭凯凰凳凶凹出击函凿刀刁刃分切刈刎刑划刖列刘则刚创初判利别刮到制刺刻刽剁剂削前剐剑剔剖剜剥剧剩剪副割剽剿劈力劝办功加务劣动助努劫劬劭励劲劳劾势勃勇勉勋勑勒勖勘募勤勺勾勿匄包匆匍匐化北匙匝匠匡匣匪匮匹区医匿十千升午半华协卑卒卓单卖南博卜卞占卢卣卤卦卧卫卯印危即却卵卷卸卿厄厅历厉压厌厔厕厘厚原厢厥厦厨厮去县参又叉及友双反发叔取受变叙叛叟叠口古句另叨叩只叫召叮可台叱史右叵叶号司叹吁吃各合吉吊同名后吏吐向吓吕君吝吞吟吠否含听启吴吸吹吻吼吾呀呆呈告呐呕员呜呦周味呵呻呼命咆和咎咏咐咒咛咥咨咫咬咸咽哀品哂哄哉响哑哙哥哨哩哭哮哲哺哽唆唇唐唤唬唯唱唾唿商啕啖啜啸啼喂喃善喈喉喊喏喘喜喝喟喧喨喷喻嗓嗔嗜嗟嗣嗤嘉嘏嘤嘱嘴嘶嘹噀噎噤器噪噫噬嚎嚷嚼囊囚四回因团囧园困围囷固国图圃圆圈土圣在圭地场坂均坊坌坎坏坐坑块坚坛坞坟坠坡坤坦垂垒垓垕垛垠垢垣垦垫埃埋城域基堂堆堑堕堤堪堰堵塌塑塔塘塞填墀境墉墓墙增墟墨墩墵壁壎壑壕壤士壬壮声壳壶处备复夏夔夕外夙多夜够夤夥大天太夫夭央失头夷夸夹夺奁奂奄奇奈奉奋奎奏契奔奕奖套奚奠奢奥女奴奸好如妃妄妆妇妒妓妖妙妥妨妫妹妻妾姊始姐姑姓委姚姜姬姻姿威娄娇娘娥娩娱娴娶娼婆婉婚婢婴婿媒媚嫁嫂嫉嫌嫔嫡嫩嬉嬖嬴子孑孔孕字存孙孚孝孟季孤孥学孩孰孱孺孽宁宄宅宇守安宋完宏宓宕宗官宙定宛宜宝实宠审客宣室宥宦宪宫宰害宴宵家容宽宾宿寂寄寅密寇富寐寒寓寔寝寞察寡寤寨寮寰寸对寺寻导寿封射将尉尊小少尔尖尘尚尝尤尧尪就尸尹尺尼尽尾局层居屈屋屏屑展属屠屡履屦屯山岁岂岌岐岑岖岗岘岛岩岭岱岳岷岸峙峡峨峪峭峰峻崇崎崔崖崤崦崩嵋嵌嵩嵯嶲嶷巅巍川州巡巢工左巧巨巩巫差己已巳巴巷巽巾币市布帅帆师希帏帐帕帖帘帙帚帛帜帝带席帮帷常帻帼帽幄幅幔幕幡幢干平年并幸幻幼幽广庄庆庇床序庐庑库应底庖店庙庚府庞废度座庭庵庶康庸庾廉廊廒廖廙廛廨廪延廷建开异弃弄弇弈弊式弑弓引弗弘弛弟张弥弦弩弯弱弹强弼归当录彘彝形彤彦彧彩彪彬彭彰影彷役彻彼往征徂径待徇徊律徐徒得徘徙徜御徨循徭微徵德徼徽心忄必忆忌忍忒忖志忘忙忝忠忤忧快念忻忽忿怀态怅怆怎怏怒怕怖怜思怠急性怨怪怬怯总怿恁恂恃恋恍恐恒恕恙恢恣恤恨恩恪恬恭息恰恳恶恸恺恼恽悃悄悉悌悍悒悔悖悚悟悠患悦悬悯悲悴悼情惆惇惊惑惕惚惜惟惠惧惨惫惭惮惯惰想惶惹愀愁愆愈愉愍意愕愚感愠愤愧愬愿慄慈慌慎慑慓慕慢慧慨慰慵慷憎憔憩憾懈懊懋懑懒懔懦懿戆戈戊戌戍戎戏成我戒戕或战戚戟戢截戮戳戴户戾房所扁扇扈扉手才扎扑扒打托扛扣执扫扬扭扮扯扰扳扶批扼承抄把抑抒抓投抖抗折抚抛抟抠抡抢护报披抬抱抵抹押抽拂担拆拈拉拊拍拒拓拔拖拗拘拙拚招拜拟拣拥拦拨择括拭拯拱拳拴拷拼拽拾拿持挂指按挑挖挝挞挟挠挡挣挤挥挨挫振挺挽捆捉捍捎捏捐捕捞损换捧据捱捶捷捻捽掀授掉掌掎排掖掘掠探掣接控推掩措掬掳掷掾揎描提插揖握揣揪揭揲援揽搀搂搅搏搔搜搠搦搪搬搭搴携摄摆摇摔摘摧摩摸撄撇撑撒撚撞撤撩播撰撺撼擂擅操擎擐擒擞擢擦攀攒攘支收攸改攻放政故效敌敏救敕敖教敛敝敞敢散敦敬数敲整敷文斋斌斐斑斗料斛斜斟斡斤斥斧斩斫断斯新方於施旁旂旄旅旆旋旌族旒旗旙无既日旦旧旨早旬旰旱时旷旺旻昂昃昆昊昌明昏易昔昕星映春昧昨昭是昱昴昶昼显晃晋晌晏晒晓晔晙晚晤晦晨普景晴智暂暇暑暖暗暨暮暴暹曜曩曰曲更曷曹曼曾替最月有朋服朐朔朕朗望朝期朦木未末本札术朱朴朵机朽杀杂权杆杉李材村杖杜束杠条来杨杭杯杰杳杵松板极构枉析枒枕林枚果枝枢枣枪枫枭枯枰枳架枷枹柄柏某柑染柔柜查柩柯柱柳柴栅标栈栉栊栋栎栏树栖栗校株样核根格栽桀桂桃案桌桐桑桓桔桥桧桨桩桶梁梅梆梓梢梦梧梨梯械梳检棉棋棍棒棘棚森棹棺椁椅植椒椽楙楚楞楫楮楯楷楼概榆榇榛榜榻槁槊槎槛槽樊樗模横樯樱樵樽橹檀檄欠次欢欣欤欲欷欺款歃歆歇歉歌歔止正此步武歹死歼殁殂殃殄殆殉殊残殒殓殖殚殛殡殪殴段殷殿毁毅毋母每毒毓比毕毖毗毙毛毡毫氅氏民气氛水永汁求汇汉汗汙汜汝江池污汤汪汰汲汶汹沂沃沅沆沈沉沌沐沓沔沙沛沟没沥沦沧沪沮沱河沸油治沽沾沿泄泉泊泌法泗泛泞泠波泣泥注泪泯泰泸泼泽泾洁洋洒洗洛洞津洪洮洱洲洹活洽派流浅浆浇浊测济浑浓浔浙浚浦浩浪浮浴海浸浼涂消涉涌涎涓涕涛涝涟涣润涧涨涪液涿淄淆淇淋淑淘淝淡淫淮淯深淳混淹添清渊渎渐渑渔渗渚渝渠渡渤温渭港渴游渺湄湖湘湫湿溃溅源溜溟溢溪溯溺溽滂滋滏滑滔滕滚滞满滥滨滩滴漆漏漓演漠漫漯漳潘潜潢潭潮潸潺潼澄澜激濛濡濦濬濮濯瀣灌火灭灯灰灵灶灸灼灾灿炉炎炜炫炬炭炮炳点烂烈烘烛烝烟烦烧热烹烽焉焕焙焚焦焫焰然煌煎照煨煮煽熊熙熟熬燃燎燕燥爨爪爬爰爱爵父爷爻爽牁牂片版牌牒牙牛牝牟牡牢牧物牲牵特牺犀犊犍犒犬犭犯状犹狂狄狈狎狐狗狝狠狡狩独狭狮狱狼猇猊猎猖猛猜猝猥猪献猾猿獐獗獠獬獭玄率玈玉王玠玩环现玲玷玺玻珍珑珝珠珩珪班球琅理琊琐琢琦琪琬琮琰琳琴琼瑁瑕瑚瑛瑜瑞瑟瑯瑶瑾璃璆璇璋璜璝璧璩璿瓒瓘瓜瓦瓮瓯瓶甄甑甘甚甜生甥用甫甬田由甲申电男甸画畅畋界畏畔留畜略番畯畴畿疆疏疑疗疠疢疥疫疮疲疴疼疽疾病症痈痊痒痕痛痢痴瘁瘠瘤瘦瘴癖癞癣癸登白百皂的皆皇皈皋皎皓皖皮皿盂盆盈益盍盏盐监盒盔盖盗盘盛盟盩目盱盲直相盼盾省眇眉看眙真眠眦眩眭眷眸眺眼着睁睚睛睡督睦睨睬睹睿瞋瞑瞒瞪瞰瞳瞻矍矛矜矢矣知矩矫短石矴矿砀砂砌砍砖砚破砾硕硝硫硬确碌碍碎碑碗碣碧碱碾磋磐磨磾示礼社祀祁祈祎祐祖祚祜祝神祠祢祥票祭祯祷祸祺禀禁禄禅福禧禳禹离禽禾秀私秃秉秋种科秘租秣秦秩秬秭积称移秽稀程稍税稔稚稠稳稷稻稼稽稿穆穰穴究穷穹空穿突窃窄窍窗窘窜窝窟窠窥窦立竖站竞竟章竣童竭端竹竺竿笃笄笑笔笙笛笠符第笮笳笺笼等筋筏筑答策筛筮筵筹简箕算管箧箪箭箱箸篁篆篇篙篡篱篷簇簏簧簪簿籍米类粉粒粗粝粟粥粪粮粱粲粹精糊糜糟系紞素索紧紫累絏絮綝繁繇纂纛纠纡红纣约级纪纬纭纮纯纱纲纳纵纶纷纸纹纽线绁练绅细织终绊绍绎经绑绒结绕绘给绛络绝绞统绢绣绥绦继绩绪绫续绮绰绲绳维绵绶综绽绿缀缄缆缉缎缓缔缕编缘缙缚缝缟缠缢缣缧缨缩缭缮缰缴缵缺罄罐网罔罕罗罚罡罢罩罪置署罹罾羁羊羌美羕羞羡群羲羸羹羽羿翁翅翊翎翔翘翟翠翩翰翱翳翻翼耀老考者耆而耐耑耒耕耗耳耶耸耻耽耿聊聋职联聘聚聪肃肆肉肋肌肓肖肘肚肝肠股肢肤肥肩肯肱育肴肺肿胀胁胃胄胆背胎胖胜胞胡胤胥胧胪胯胶胸能脂脉脊脍脏脐脑脚脯脱脸腊腐腑腔腕腥腮腰腴腹腾腿膀膂膊膏膑膛膝膳膺臂臣臧自臭至致臻臼臾舄舅舆舌舍舒舜舞舟般舰舱舵舸船艘艟艨良艰色艳艺艾节芒芜芝芟芥芦芬花芳芸芽苇苌苍苏苑苒苗苛苞苟若苦英苴苹茂范茅茔茕茨茫茬茵茶荀荆草荏荐荒荡荣荥荧荩荫药荷荻荼莅莒莘莞莩莫莱莲获莹莽菜菲菽萁萃萌萤营萧落葆著葛董葫葬葭葺蒂蒋蒙蒜蒯蒲蒸蒹蒺蒿蓄蓍蓝蓦蓬蔑蔓蔚蔡蔬蔺蔽蕃蕤蕲蕴薄薛薤薨薪薰藁藉藏藐藜藤藩蘸蘼虎虏虐虑虔虚虞虢虫虬虹虽虾虿蚁蚕蛇蛉蛙蛛蛟蛮蛱蜀蜂蜈蜉蜘蜜蜺蜾蝇蝉蝎蝗蝣蝶蝼螂融螟螳螺蟊蟠蠡蠢蠫蠹血衄衅行衍衔街衙衠衡衢衣补表衫衬衮衰衷衽衿袁袄袅袋袍袒袖袛袤被袭裁裂装裒裔裘裙裤裨裴裸裹裾褐褒褚褥襄襟西要覆覈见观规觅视览觉觐觑角觜觞解觥触觫觳言誉誓譔警譬计订讣认讥讨让讪讫训议讯记讲讳讴讶许讹论讼讽设访诀证评识诈诉诊诌词诏试诗诘诚诛话诞诠诡询诣诤该详诩诫诬语误诰诱诲诳说诵请诸诹诺读课谀谁调谄谅谈谊谋谌谍谏谐谑谒谓谕谗谘谙谚谛谞谡谢谣谤谥谦谧谨谪谬谭谮谯谱谲谴谶谷豁豆豕豚象豨豪豫豸豹豺貂貅貌貔賨贝贞负贡财责贤败货质贩贪贫贬购贮贯贰贱贲贴贵费贺贻贼贽贾贿赀赂赃资赈赋赌赍赎赏赐赖赘赚赛赞赠赡赢赤赦赧赫赭走赴赵赶起趁超越趋趱足趷跃跄跋跌跎跑跖跛距跞跟跣跨跪路跳践跶跸跼踅踉踊踌踏踞踢踪踵蹄蹇蹈蹉蹊蹋蹐蹑蹙蹬蹶躁躄躇躔身躬躯躲轘车轨轩转轮软轰轲轴轵轸轻载轿辂较辄辅辆辇辈辉辍辎辑输辔辕辖辛辜辞辟辨辩辰辱边辽达迁迂迄迅过迈迎运近迓返还这进远违连迟迢迤迩迫迭述迷迸迹追退送适逃逅逆选逊逍透逐递途逗通逝逞速造逡逢逦逮逵逶逸逻逼逾遁遂遇遍遏遐遑道遗遣遥遨遭遮遵遽避邀邂邈邑邓邕邙邢那邦邪邮邯邰邱邳邴邵邸邹邺邻邽邾郁郃郊郎郏郑郗郝郡郢郤郦部郭郯郸都郿鄂鄄鄙鄠鄢鄣鄱酂酉酋酌配酎酒酗酣酥酬酱酷酸酹酾酿醉醒醢醮醴采释里重野量金釜釭釿鈇鈚鉴銮鍪鏖钁针钉钓钗钝钟钢钦钧钩钱钳钵钺铁铃铜铠铨铭铮银铸铺链销锁锄锅锋锐错锡锢锣锤锥锦锭键锯锵锹镇镌镔镜镝镫镬镰镶长闇门闪闭问闲间闵闷闸闹闺闻闼闾闿阁阃阄阅阆阉阊阍阎阐阑阔阖阙阚阜队阡阪阱防阳阴阵阶阻阿陂附际陆陇陈陋陌降限陕陛陟院除陨险陪陵陶陷隅隆隈随隐隔隗隘隙障隧隰隳隶隽难雀雁雄雅集雉雌雍雏雒雕雝雠雨雪雳零雷雹雾需霁霄霆震霍霎霏霖霜霞露霸霹青靓靖静非靠靡面革靬靳靴靶鞅鞍鞒鞘鞠鞦鞭韦韧韩韪韬音韵韶顒顗顶顷项顺须顽顾顿颀颂预颅领颇颈颊颌颍颐频颓颔颖颗题颜额颠颤风飏飐飘飙飞食飨餐餮饕饥饬饭饮饯饰饱饵饶饷饼饿馀馁馆馈馑馒馔首馗馘香馥馨騕马驭驮驯驰驱驴驷驸驹驻驼驽驾驿骁骂骄骅骆骇骈骋验骏骑骖骘骚骝骞骠骡骤骥骧骨骸髀髓高髡髦髫髭髯髻鬃鬅鬓鬯鬻鬼魁魂魄魅魇魏鱼鲁鲂鲈鲍鲜鲧鲲鲵鲸鳅鳌鳖鳝鳞鸟鸠鸡鸣鸦鸩鸭鸯鸳鸷鸾鸿鹄鹅鹉鹊鹏鹗鹤鹦鹩鹪鹭鹰鹿麈麋麒麟麦麴麻麾黄黍黎黑默黛黜黥黩黯鼎鼐鼓鼕鼙鼠鼻齁齐齑齿龄龙龚龛龟！（），：；？\n",
      "不同字符数量: 3,951\n"
     ]
    }
   ],
   "source": [
    "# 计算《三国演义》中有多少个不同的字符\n",
    "\n",
    "# set去重 --> list转换成列表 --> 排序\n",
    "chars = sorted(list(set(data)))\n",
    "# 不同字符的数量\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"所有不同的字符:\", ''.join(chars))\n",
    "print(f\"不同字符数量: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "828aae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3934\n"
     ]
    }
   ],
   "source": [
    "# 创建从字符到整数的映射\n",
    "\n",
    "# 从字符到整数的映射字典\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# 从整数到字符的映射字典\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# 例如我们可以看一下`鼻`这个字对应的整数\n",
    "print(stoi['鼻'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "40174f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2044, 2044, 3600, 1881, 40, 3429, 1871]\n",
      "滚滚长江东逝水\n"
     ]
    }
   ],
   "source": [
    "# 给定一个字符串`s`, 输入字符串中每个字对应的整数组成的列表\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "# 给定一个整数列表, 返回列表中每个整数对应的字符所组成的字符串\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# 测试一下\n",
    "print(encode('滚滚长江东逝水'))\n",
    "print(decode([2044, 2044, 3600, 1881, 40, 3429, 1871]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "acf39fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据集\n",
    "# 将《三国演义》前90%的文字作为训练数据集\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "# 将《三国演义》后10%的文字作为验证数据集\n",
    "val_data = data[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "454b33ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集中有 544,993 个字符(token)\n",
      "验证数据集中有 60,555 个字符(token)\n"
     ]
    }
   ],
   "source": [
    "# 分别将训练数据集中的字符和验证数据集中的字符编码成整数\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "\n",
    "print(f\"训练数据集中有 {len(train_ids):,} 个字符(token)\")\n",
    "print(f\"验证数据集中有 {len(val_ids):,} 个字符(token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b54200bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练数据集和验证数据集分别保存成二进制文件\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('train.bin')\n",
    "val_ids.tofile('val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "7d2608a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将元数据保存成pickle格式的文件, 供我们后面在encode或者decode时使用\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open('meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e13928",
   "metadata": {},
   "source": [
    "我们数据准备的工作就完成了.\n",
    "\n",
    "# 编写GPT模型\n",
    "\n",
    "接下来我们开始编写模型代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e4828489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入需要的一些包\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e5c01",
   "metadata": {},
   "source": [
    "## GeLU激活函数\n",
    "\n",
    "公式如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "24be4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义GELU激活函数, 具体论文参见:\n",
    "# https://arxiv.org/abs/1606.08415\n",
    "def new_gelu(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0693f30",
   "metadata": {},
   "source": [
    "## 层归一化模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2c5f08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义层归一化模块\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0c99f",
   "metadata": {},
   "source": [
    "## 因果自注意力机制模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "509cc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 确保词嵌入向量的维度是head数量的整数倍\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # 下面的线性变换计算的是:\n",
    "        # 在一批中, 所有头的key, query, value的投影\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # 正则化(regularization)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        # 下面的线性变换的作用是将投影输出\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # head的数量\n",
    "        self.n_head = config.n_head\n",
    "        # 词嵌入向量的维度\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        定义因果自注意力模块在接收到张量x时, 输出什么样的张量\n",
    "        \"\"\"\n",
    "        B, T, C = x.size() # 批的大小(batch size), 序列长度(sequence length), 词嵌入向量维度(n_embd)\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        \n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d9313",
   "metadata": {},
   "source": [
    "## 多层感知机模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "7a5ae02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = new_gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a159a8f",
   "metadata": {},
   "source": [
    "## Block模块\n",
    "\n",
    "![](assets/Block模块示意图.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3e8a12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f322b74",
   "metadata": {},
   "source": [
    "## GPT模型的一些参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c1ef3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798e476",
   "metadata": {},
   "source": [
    "## GPT模型的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3ee3b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        # Transformer模块\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # 初始化所有权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # 打印模型的参数数量\n",
    "        print(\"参数数量: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "        \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.2)\n",
    "                \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"无法前馈(向前发送)序列长度: {t}, 因为block size只有{self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        \n",
    "        tok_emb = self.transformer.wte(idx) # token嵌入向量的形状 (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # 位置嵌入向量的形状 (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n",
    "        # will appear in the no_decay and decay sets respectively after the above.\n",
    "        # In addition, because named_parameters() doesn't return duplicates, it\n",
    "        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n",
    "        # so let's manually remove 'lm_head.weight' from decay set. This will include\n",
    "        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n",
    "        decay.remove('lm_head.weight')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd31d",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "\n",
    "由于配置问题, 我们使用CPU来训练GPT模型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "219e6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "eval_interval = 250\n",
    "log_interval = 1\n",
    "eval_iters = 20\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "# data\n",
    "gradient_accumulation_steps = 5 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 64\n",
    "# model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 128\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.99\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "lr_decay_iters = 2000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# system\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "gradient_accumulation_steps *= 8 # simulate 8 gpus\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cpu' # for later use in torch.autocast\n",
    "ctx = nullcontext()\n",
    "\n",
    "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "ae5fd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义切分数据集为输入和标签的函数\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "3abc3476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 3951 (inside 'meta.pkl')\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0 # 迭代次数\n",
    "best_val_loss = 1e9 # 最佳损失\n",
    "\n",
    "with open('meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "meta_vocab_size = meta['vocab_size']\n",
    "print(f\"found vocab_size = {meta_vocab_size} (inside 'meta.pkl')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "63b363f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从头开始训练模型\n",
      "参数数量: 1.29M\n",
      "using fused AdamW: False\n",
      "忧臣辱。某久事袁氏，岂可背之！”操知其不可留，乃遣回。评回见谭，言操不准投降。谭叱曰：“汝弟现事曹操，汝怀二心耶？”评闻言，气满\n",
      "臣辱。某久事袁氏，岂可背之！”操知其不可留，乃遣回。评回见谭，言操不准投降。谭叱曰：“汝弟现事曹操，汝怀二心耶？”评闻言，气满填\n"
     ]
    }
   ],
   "source": [
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=meta_vocab_size, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "# 从头开始训练模型\n",
    "print(\"从头开始训练模型\")\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "model.to(device)\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "print(decode(X[1].tolist()))\n",
    "print(decode(Y[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "762b70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估算损失\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c7c3b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e9ff04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 8.3115, val loss 8.3091\n",
      "iter 0: loss 8.3285, time 2750.88ms\n",
      "iter 1: loss 8.3125, time 1935.22ms\n",
      "iter 2: loss 8.3172, time 1945.68ms\n",
      "iter 3: loss 8.3049, time 1922.65ms\n",
      "iter 4: loss 8.3172, time 1912.10ms\n",
      "iter 5: loss 8.2958, time 2022.62ms\n",
      "iter 6: loss 8.3068, time 2186.26ms\n",
      "iter 7: loss 8.3106, time 2258.78ms\n",
      "iter 8: loss 8.3074, time 2028.46ms\n",
      "iter 9: loss 8.3037, time 2096.54ms\n",
      "iter 10: loss 8.2843, time 2063.59ms\n",
      "iter 11: loss 8.3023, time 2249.82ms\n",
      "iter 12: loss 8.2935, time 2345.97ms\n",
      "iter 13: loss 8.2923, time 2577.94ms\n",
      "iter 14: loss 8.2826, time 2464.92ms\n",
      "iter 15: loss 8.2893, time 2327.37ms\n",
      "iter 16: loss 8.2691, time 2392.97ms\n",
      "iter 17: loss 8.2718, time 2459.26ms\n",
      "iter 18: loss 8.2606, time 2268.73ms\n",
      "iter 19: loss 8.2513, time 2492.58ms\n",
      "iter 20: loss 8.2384, time 2398.60ms\n",
      "iter 21: loss 8.2184, time 2443.73ms\n",
      "iter 22: loss 8.2092, time 2283.98ms\n",
      "iter 23: loss 8.2103, time 2434.89ms\n",
      "iter 24: loss 8.1674, time 2371.96ms\n",
      "iter 25: loss 8.1190, time 2269.50ms\n",
      "iter 26: loss 8.1048, time 2279.16ms\n",
      "iter 27: loss 8.0693, time 2389.01ms\n",
      "iter 28: loss 7.9965, time 2251.90ms\n",
      "iter 29: loss 7.9640, time 2302.29ms\n",
      "iter 30: loss 7.8817, time 2391.18ms\n",
      "iter 31: loss 7.8040, time 2256.26ms\n",
      "iter 32: loss 7.7365, time 2301.91ms\n",
      "iter 33: loss 7.6558, time 2350.55ms\n",
      "iter 34: loss 7.5891, time 2375.37ms\n",
      "iter 35: loss 7.5495, time 2391.11ms\n",
      "iter 36: loss 7.5165, time 2506.54ms\n",
      "iter 37: loss 7.4496, time 2344.87ms\n",
      "iter 38: loss 7.3969, time 2370.14ms\n",
      "iter 39: loss 7.4022, time 2474.17ms\n",
      "iter 40: loss 7.3249, time 2380.33ms\n",
      "iter 41: loss 7.2506, time 2394.55ms\n",
      "iter 42: loss 7.2064, time 2378.63ms\n",
      "iter 43: loss 7.2090, time 2354.91ms\n",
      "iter 44: loss 7.1915, time 2307.92ms\n",
      "iter 45: loss 7.0647, time 2276.90ms\n",
      "iter 46: loss 7.0454, time 2265.74ms\n",
      "iter 47: loss 7.0342, time 2294.22ms\n",
      "iter 48: loss 7.0399, time 2282.03ms\n",
      "iter 49: loss 6.9314, time 2388.47ms\n",
      "iter 50: loss 6.9071, time 2345.62ms\n",
      "iter 51: loss 6.8555, time 2358.56ms\n",
      "iter 52: loss 6.8071, time 2370.06ms\n",
      "iter 53: loss 6.8257, time 2326.41ms\n",
      "iter 54: loss 6.7791, time 2346.47ms\n",
      "iter 55: loss 6.7484, time 2310.74ms\n",
      "iter 56: loss 6.6752, time 2321.84ms\n",
      "iter 57: loss 6.5426, time 2355.56ms\n",
      "iter 58: loss 6.6137, time 2470.82ms\n",
      "iter 59: loss 6.6588, time 2355.80ms\n",
      "iter 60: loss 6.5399, time 2508.14ms\n",
      "iter 61: loss 6.4410, time 2360.58ms\n",
      "iter 62: loss 6.5514, time 2469.45ms\n",
      "iter 63: loss 6.4114, time 2457.35ms\n",
      "iter 64: loss 6.5107, time 2421.96ms\n",
      "iter 65: loss 6.3692, time 2351.21ms\n",
      "iter 66: loss 6.3440, time 2343.78ms\n",
      "iter 67: loss 6.3886, time 2277.28ms\n",
      "iter 68: loss 6.2930, time 2281.42ms\n",
      "iter 69: loss 6.5071, time 2263.42ms\n",
      "iter 70: loss 6.3363, time 2245.56ms\n",
      "iter 71: loss 6.2796, time 2249.25ms\n",
      "iter 72: loss 6.4254, time 2363.53ms\n",
      "iter 73: loss 6.2659, time 2275.64ms\n",
      "iter 74: loss 6.2973, time 2261.83ms\n",
      "iter 75: loss 6.2100, time 2240.34ms\n",
      "iter 76: loss 6.0829, time 2249.34ms\n",
      "iter 77: loss 6.3004, time 2256.36ms\n",
      "iter 78: loss 6.1674, time 2253.48ms\n",
      "iter 79: loss 6.3420, time 2302.04ms\n",
      "iter 80: loss 6.3172, time 2260.74ms\n",
      "iter 81: loss 6.2487, time 2322.50ms\n",
      "iter 82: loss 6.1721, time 2317.04ms\n",
      "iter 83: loss 6.1951, time 2331.12ms\n",
      "iter 84: loss 6.1815, time 2260.52ms\n",
      "iter 85: loss 6.2669, time 2261.20ms\n",
      "iter 86: loss 6.1185, time 2264.85ms\n",
      "iter 87: loss 6.1350, time 2256.45ms\n",
      "iter 88: loss 6.1723, time 2301.15ms\n",
      "iter 89: loss 6.3485, time 2291.91ms\n",
      "iter 90: loss 6.0845, time 2757.17ms\n",
      "iter 91: loss 6.1583, time 2420.72ms\n",
      "iter 92: loss 6.2489, time 2511.09ms\n",
      "iter 93: loss 6.2263, time 2315.61ms\n",
      "iter 94: loss 6.1908, time 2319.02ms\n",
      "iter 95: loss 6.3419, time 2331.25ms\n",
      "iter 96: loss 6.3737, time 2311.64ms\n",
      "iter 97: loss 6.2298, time 2308.18ms\n",
      "iter 98: loss 6.2160, time 2294.73ms\n",
      "iter 99: loss 6.2051, time 2392.01ms\n",
      "iter 100: loss 6.2596, time 2462.01ms\n",
      "iter 101: loss 6.2593, time 2479.65ms\n",
      "iter 102: loss 6.1889, time 2361.57ms\n",
      "iter 103: loss 6.2992, time 2282.06ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[332], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m micro_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_accumulation_steps):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 36\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[325], line 61\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m---> 61\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[323], line 10\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[321], line 26\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m B, T, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;66;03m# 批的大小(batch size), 序列长度(sequence length), 词嵌入向量维度(n_embd)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# calculate query, key, values for all heads in batch and move head forward to be the batch dim\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m q, k ,v  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, nh, T, hs)\u001b[39;00m\n",
      "File \u001b[0;32m~/machine-learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/machine-learning/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "\n",
    "# 训练代码\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"将模型的检查点文件保存到 'ckpt.pt'\")\n",
    "                torch.save(checkpoint, 'ckpt.pt')\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        lossf = loss.item() # loss as float. note: this is a CPU-GPU sync point\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1f4b3",
   "metadata": {},
   "source": [
    "# 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "60a44c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数数量: 1.29M\n",
      "\n",
      "昔令马于大死曰：“非孙操　　　却有吴，即德曰：“既言。”操命曰：“张操曰：“然大。”遂在：“孔明不可不曰：“吾以自遣无等要见？”庞德不令出？：“汝何将请王不汉曰：“吾以某近曰：“汝说公曰：“必不乃能退，并拜先闻何将，吾有是也，今亦毕，故日曰：“吾者视者。”遂多可，如言。”睿曰：“将，必说诸说诸从，以言，便在功，张操告意：“吾之，反之。”自不将许夫一相有延取今尚必退不知。”孔明汝说尚云以延如兵，愿故人，留相欲受军接，待将。”张明曰：“此！”次是兵。”玄德长子已进，便欲能无是备曰：“吾弟有可将乃用大不人曰：“又弟，必若有吾在。”遂欲不主将后先日，反？”众已得，刘孔　明曰：“，令。”玄德主之皆，方见前多。”且吾公无可多，若肃曰：“叱曰：“闻玄德为何如及不十道中不来。”孔操曰：“一曾人，恐、公马也。”孔布曰：“玄德闻父士，早能被魏一可先人间，主后其可兵，吾得来？”玄德乃在吾日意。背云。\n",
      "　　　　　　　　　　　\n",
      "　明汝知是相报延？”且何欲二是此在太亮相引是汝今更曰：“汝说王马，吾将一将兵也。吾不有等来公用之便听取主何人，今瑜安看。”公与我到。今见来。”绍引我大锋兵，然，有江人为夫于葛，乃瑜、刘瑜\n",
      "---------------\n",
      "\n",
      "今欲士，可唤主主视将徐，出，望来有喜。后：“蜀从长多意，若可为此下玄德告，一一杀，大可当军，不思军，不大定。懿；。”谁，上见上，主引军，不闻关之军二有东书，即长公一发，不得来，与太之？后言。吾无夜，只引见军？”玄姜亮必言于，当且复今不取说：“此二，皆在城首，背瑜曰：“吾急，非知后。”且此领兵，何，可然曰：“吾诸军不可辞至，何在此之？”玄德、绍、吾曰：“袁孔　次行。”融曰：“此人一知，不可杀，皆在手，纵官曰：“汝只报说袁正在喜，以喝自为子州曰：“说权之。”吾报臣之战，先五才分曰：“说我，马曰：“赵仁书，必兄亲曰：“孔此兵，汝肃曰：“必弟，亦令，以吾日曰：“玄明曰：“吾见也。”玄德曰：“是吾吾欲欲令若何欲有千，何要又与张原军于之袁公曰：“吾如待出，乃何人，乃主超一敢长引人。今正使王一十之，我曹统曰：“欲有要大问、张玄德，不惊，回。”苞不得至武伏曰：“蜀也，我入而得之处之之，乃说其入无如说此马来，不当在后奔赶见后言，方杀与不知。”何引前，且一人。”遂曰：“公曰：“遂不再能人曰：“瑜！”辽、张操曰：“孔德曰：“四知兵报鲁、孟曰：“闻不将弟报何知。”遂公毕，公之他长命商之。”\n",
      "　　　　　　　　　\n",
      "---------------\n",
      "\n",
      "\n",
      "却有即时，自生，孔，吾故有路，岂军已后之！“不以不生。”公问、玄曰：“昨、张玄？”瑜令知不可不”国也，何三来之贼命，君曹孔德曰：“正言弟玄德曰：“他只见兄自既引军在无既与不大死乎，不非说孔明曰：“既说备，不不将蜀将得王武蜀者，某从，吾人，无天曰：“见。”操欲从刘表曰：“今吾何不我官曰：“吾不死也。”明命，与如尚引兵军令公，曹庞明曰：“吾当败。”玄德曰：“不走。”爽曰：“肃曰：“玄明曰：“汝二来为此公与四小敬亦留来，某先笑接？”玄德之。”大知，只相去，令兵，汝不说权曰：“长，皆毕，何合出军将兵弟，欲如，当，引闻非令孟：“吾来曰：“且人而先姓大得命曰：“孤。”某，蜀进出，汝胜，使，谓丕曰：“说：“自遇之；陆昱曰：“玄德曰：“孙魏今说韩知孙操曰：“东十之军曰：“若说夏夫人？”云、那回应。吾兄，引大可引左敬，即未既以以乃不先作？”孔明，大令三如吾吾、昔人救，同，恐公矣。”布无命兵曰：“曹明曰：“说张瑜曰：“赵绍之！”皇，如二决侯大事。”玄明，日，乃此公？”操者，不出败军在如何退？”操曰：“愿。”操曰：“当不说玄德曰：“吾用，共曰：“张孔明亲用，那只就汉，不相至，见，急引大何引来。”荀叹大将有\n",
      "---------------\n",
      "\n",
      "肃曰：“今汝”明如吾云曰一轻在万。”孔明计，先可急！”肃曰：“何所得军蜀得未遣长，当在张之！”玄德乃只有喜！”吾？”操此到寨之孔操，今汝孙杨日！\n",
      "\n",
      "　　明曰：“说孔操曰：“吾、此急杀，必故乃可下东军，必岂去，急曰：“二有怒曰：“刘文侯，说平可人。”吾得擒关公曰：“何是江，马去。”不宜为前，只安等公曰：“可教一知一十谷，故？”魏于人，某闻说袁明，急吴。”某不说后不延曰：“正吾急，可复笑曰：“有谋之。吾命为知五队，不人曰：“非不能见孔操曰：“袁遂我，袁先以五骑，我如某有如言！”张云将事。孔　　　　明言也。”卓曰：“吾何，亦休为其无说玄德曰：“孔　云不得来，吾说吾闻将相然，可，故有如失曰：“非何大此相皆领入我有何闻延之我引，上之？”忠，两计时也，休人到马，言，可言如闻来。马不与吾一思天，不知应一得一可与魏此。东何曰：“必然人曰：“遂追到定。”曹吾若知破，今延曰：“吾玄德也，愿兄曰：“言。”若何如喜，故不不？”操大将报刘汝何以何大是关曰：“曹操曰：“我曰：“吾张表曰：“真等其教有如说。”此主已来，果听书，”先闻吾正大说关既。”孔明为何言，不知日，乃言。”贤也，瑜曰：“使见急。”备曰：“先可知报\n",
      "---------------\n",
      "\n",
      "\n",
      "又兴人回，兵公，望此，\n",
      "\n",
      "\n",
      "\n",
      "　　\n",
      "　\n",
      "　　　却是吾乃知闻今已多言曰：“何再肃等报忠至也。”谁““瑜相人吾等，吾既主之人以军前，不与汝已到。汝此自大人，不无可使后一一及以将二年败，忽故急，自回一安得云相日东，一人有帝，孔飞领去，张荀刘后去。”\n",
      "\n",
      "\n",
      "　明子坐之，以说贤取主，约军刘乾言？”玄德，乘于言进，大诗路，何使书，自可随肃，见，方守我此。\n",
      "\n",
      "　　　\n",
      "\n",
      "　　　　　　\n",
      "　　　却是此，遣事，若使日上军，去。”孔明同之计曰：“汝相便不日问言也，遂为又可公，至军出前见军死事，大非说一人，公为不再自自在三能如使报：“故起，言相待，皆军。早急，乃为事。”将出，乃安二为丞吾言曰：“日之黄子贼。\n",
      "　却说丞欲言来，有葛枪，不能不将军，说今司，是主相皆兵，即乘用，反去，如二去，乃军？”布兵计；先可报。”公得：“若十，乃国进，则之：“今不更有张玄德、关公相等故军，不肯，大若何行余起；：“云，不才，各报今何令。”遂出门之事？”飞，下生，何，于马至心，将行，不分其来，各不数乘之，天其在贼。操问曰：“庞德遣破不日矣，不说又然懿心，吾引军，背。”曹权入、蔡文一领，须如兵，何，不可无可得之，为玄德曰：“某随为如此。\n",
      "---------------\n",
      "\n",
      "　生回曰：““如将，后大“叔曰：“庞明曰马不敢令也，先行！”允相见人又相领一有说说一何言其相可谓也曰：“夫有曹因：“不“汝有日，且取前之。”鲁大决于军兵，不有如使，必见中之操，此战，遂往见。今言公曰：“某与曹曹公公如不往不日先可，乃可人大以往曹权拜长马，\n",
      "　　明曰：“说魏将数知董、我守今此，曹操又是小马，岂可人之 乎？”先日，何自如、其也。”懿，不将乃使，如之！”\n",
      "　　却何后，以孔明曰：\n",
      "\n",
      "\n",
      "　　　明曰：“说汝可人：“于只说一子不将将令后。吾：“曹明曰：“可说司之孔明下报吾入人；其军？”张孔明便二某公，唤也，被汝不知事。”吾先取此，瑜曰：“汝与李仪去军领孙玄德曰：“孙张飞与卿曰：“此超曰：“某在延辞同应，吾命发，子之杀之，乃非事，用军而见，吾今瑜曰：“非张吾休将军三人有日曰：“与军。今吴，又望大我我：“我亲既不相从，孔明曰：“、飞曰：“即令懿某已，吾用是孙权如说昔与其大人等后去，大喜曰：“备也？：“汝孙丕、兵下至，不得长、皇闻前来之？”操与吾曰：“但便下平曰：“获曰：“玄德言在若日，乃何有日中之马，言也。”玄德不识之，孔明曰：“兄言、玄德曰：“玄德公曰：“虽日太其可然中，说公兵等，被张\n",
      "---------------\n",
      "\n",
      "\n",
      "德至有肯。周、江而，臣下之。至马入，不人，即官为百回人，非吾不说。”\n",
      "　　　　\n",
      "　未处”孙玄德，曰：“张备，此！”曹：“瑜甚乃知见而皆不死之，必兄曰：“延曰：“吾相以惊讫。”表等。若吾公遂不能得起之众日，又心，不百事，忽某不日书曰：“玄德曰：“此他引，吾将三在将长为日，某命、我出者之，张布，之来！”此乃是刘云引言，不欲当天等如魏不请言，不人不恐公曰：“且喝之自入。”玄德相，尚追后兵见以可必来后行中有说，必肯曰：“表等人归至前，名子，长之，一欲见，即可轻来，可处，关吾平将往。”瑜不听立此为我。今何此又有军杀人。”孔明曰：“刘臣于夏典天战入之之。正令 之，以在兵。”军入。”权曰：“此相可非当？”张丕曰：“孔明。”瑜曰：“且不在来已进无大必欲见，孔明曰：“汝吾回二去，恐从无问既引出。”玄德，”正一乘夜甚决兵处。”汉祖之耶。”\n",
      "\n",
      "　明曰：“有在人遣无可追曹云闻吕备曰：“兵至不不知刘操大日曰：“诸说魏然兄贼？”谁取，必已谋，乃魏延不此曰：”孔孔明在哭同命不可为曹孔布问矣。”备，果可兵，何如我领士，令。”玄德大得长必三从之来，司子臣？”操即若与左宫以国臣追。两败，有敢相引来，恐得军不识之，必。张\n",
      "---------------\n",
      "\n",
      "可。操，即久人无人曹孔德公此吴。我未曾自”玄德曰：“当长曰：“云使，今夜安主又是令见。””君、权吾有教军之，如，，若此喜！”操曰：“二可日令关军！”董安人曰：“今其不得取人引后令必有是今何报主等官。”今，谁得令黄祖之。”统在公！”操曰：“张操曰：“不主以一人，往汉曰：“肃见事部不在为四与东兵，可一可相可在一余将二只不日，以有此曹权遣者。”\n",
      "统，正来，孙操曰：“玄明生二知言，可以有夫人，当曰：“此心，心弟，孤至马与若此二有可报书，吾何兵，吾曰：“愿已走？”操、曹玄德报我如既见投破，吾之。”乾也，便为人如说吾曰：“将应进大知，已与城之。吾休大人？”汝两胜报孔　　　　明？”玄德曰：“夫有取？”操闻军令之？”赵陈下文东公也东见后公曰：“此，众后，如曹操曰：“说张表回日无得命曰：“公不说军下已欲进，必不日，一军，回之！”懿，可胜曰：“汝长公未人。刘权引将是玄德曰：“非子大说曹\n",
      "　明曰：“\n",
      "\n",
      "　　　　　　　　　明，正即此军不惊曰：“吾非如汉，与张瑜令，且欲言，不主大可敢三军曰：“汝不欲人？”徐儿，吾瑜引，各然，不不说今此败之，不从，某领长看，云岂可疑，只如知杀拜以已得何送同到，遂一合，急，早处，公\n",
      "---------------\n",
      "\n",
      "\n",
      "今相便有千，可使亲，不南高刘操知兵归者，乃将懿乃人；与五使于，大　岂知有说城人，字之，　　　　　不能回军，马。说孙璋帝欲。张明收之，乃我汝不可不说城一日了。何见等相被二某便人刘\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "第四言。操受，却何欲所听大可与山州，便合，不可到，又然，回。”吾所从一合曰：“是延曰：“蜀军而可使不可子在此去曰：“鲁无知，我相何引子吾欲可为何见诸起，再各令，尽以不敢计曰：“吾来，一可能心。”即唤大人，待今汝复命必不与权领大然不死。”肃曰：“休不何惧在荆水子以？”遂引；朱酒也，愿弟？”郭侯入夫如我意已必遂相见吕正乃马。郭敬之，何，教荆仪曰：“某日曰：“于地军不笑等乃主使汉不斩而走，引公曰：“子：“玄德曰：：“司令进。今欲笑之事矣？”操闻二见言计士此。”遂不此救到，”又肃曰：“玄德曰：“天上，大可复无百下也，当退：“此？吾公到曰：“吾曰：“岂不胜！”布因且可有如为公一可归下，不人，留岱甚有吾夜公曰：“吾不领相得当兵，何此公，见孙玄德知？：“汝吾败引子下。”遂且某使军，与云闻此军得他相人之了。”孔明曰：“我只出，吾亦人曰：“日之。”封也，不是何言、张操、后以喝而人，乃老山，吾懿曰：“使二可乃即前贼者曰\n",
      "---------------\n",
      "\n",
      "\n",
      "公回以不千，孔明又。”\n",
      "　却蜀急，某相曰：“今吾见。”玄　融曰：此将于此休到，”第恐，曹操曰：“布曰：“将等孔“吾瑜大是于使不人曰：“昔可有为惊，可方能命，若是何五将，日，是赵备了众闻此为汝必能不敢一追？”将时攻三合曰：“闻夜曰：“吾军于刘布在知懿不何将于此至，此之，等可就三引，可立。但出，何无军来。”使诸不反将不里都奉军出士士、前，不见军之，是兵发出，将，因谓曹孔明乃说张布曰：“恐为我自便分东将关瑜曰：“今往，乃人，一人，望君日之，领？”操曰：“某为是吾何受人，可下。”玄德。”某也，必怒曰：“吾知。”玄德。”周有日之。”众将将孙嶷敬下下。”绣曰：“某长，二可至，以他，何走了。”布乃不令将一将见杀与延之物！”操曰：“若人在何可，复取事上，有延无见，又去了，此相有，如人人曰：“言。玄德部大将得来东，只得汉侯而人公用，我未可之。\n",
      "　　　　明曰：“此上见，命，却长不知不入曰：“孔明，可已计，乃因军之恩不待超时、法，若孙布曰：“吾某公知进，只数事军二回。”既遣自得公，可就国兵人引，吾肃曰：“魏吕权曰：“此，孔明将之。”关公相来追军，先不将超得，必不瑜相大得已时时不年之军不被关随人，令同后斩，\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "\n",
    "start = \"\\n\"\n",
    "num_samples = 10\n",
    "max_new_tokens = 500\n",
    "top_k = 200\n",
    "temperature = 0.8\n",
    "seed = 1337\n",
    "device = 'cpu'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device_type = 'cpu'\n",
    "ctx = nullcontext()\n",
    "\n",
    "checkpoint = torch.load('ckpt.pt', map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with open('meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "stoi, itos = meta['stoi'], meta['itos']\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76ca90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
