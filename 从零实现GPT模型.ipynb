{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1250a5",
   "metadata": {},
   "source": [
    "# 准备数据集\n",
    "\n",
    "我们将《三国演义》的原文作为数据集, 来训练一个字符级别的语言模型. 也就是将原文中的汉字以及标点符号等等映射成整型(`int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b87dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的包\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e415caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符数据集的长度: 605,548\n"
     ]
    }
   ],
   "source": [
    "# 打开《三国演义》的文本文件`input.txt`\n",
    "# 然后读取\n",
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 打印《三国演义》中的字符数量\n",
    "print(f\"字符数据集的长度: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ad9badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有不同的字符: \n",
      " <>[]—‘’“”…□　、。《》【】一丁七万丈三上下不与丐丑专且丕世丘丙业丛东丝丞丢两严丧个中丰临丸丹为主丽举乂乃久么义之乌乎乏乐乔乖乘乙九乞也习乡书买乱乳乾了予争事二于亏云互五井亘亚些亟亡亢交亥亦产亨亩享京亭亮亲亵亹人什仁仅仆仇今介仍从仓仔仕他仗付仙仞代令以仪们仰仲件价任仿伉伊伍伎伏伐休众优伙会伞伟传伤伦伪伯伴伷伸伺似但位低住佐佑体何佗余佛作佞你佣佥佩佯佳佻使侄侈例侍供依侠侥侧侪侮侯侵便促俄俊俎俗俘保俞俟信俦俨俭修俯俱俸俺俾倅倍倏倒倘候倚借倡倥倦值倾偃假偎偏偕做停健偬偶偷偿傅傍傕储催傲像僚僧僭僮僵僻儁儒儿兀允元兄充兆先光克免兔兖党兜兢入全八公六兮兰共关兴兵其具典兹养兼兽冀内冈册再冒冓冕冗写军农冠冢冤冥冬冯冰冲决况冶冷冻净凄准凉凋凌减凑凛凝几凡凤凭凯凰凳凶凹出击函凿刀刁刃分切刈刎刑划刖列刘则刚创初判利别刮到制刺刻刽剁剂削前剐剑剔剖剜剥剧剩剪副割剽剿劈力劝办功加务劣动助努劫劬劭励劲劳劾势勃勇勉勋勑勒勖勘募勤勺勾勿匄包匆匍匐化北匙匝匠匡匣匪匮匹区医匿十千升午半华协卑卒卓单卖南博卜卞占卢卣卤卦卧卫卯印危即却卵卷卸卿厄厅历厉压厌厔厕厘厚原厢厥厦厨厮去县参又叉及友双反发叔取受变叙叛叟叠口古句另叨叩只叫召叮可台叱史右叵叶号司叹吁吃各合吉吊同名后吏吐向吓吕君吝吞吟吠否含听启吴吸吹吻吼吾呀呆呈告呐呕员呜呦周味呵呻呼命咆和咎咏咐咒咛咥咨咫咬咸咽哀品哂哄哉响哑哙哥哨哩哭哮哲哺哽唆唇唐唤唬唯唱唾唿商啕啖啜啸啼喂喃善喈喉喊喏喘喜喝喟喧喨喷喻嗓嗔嗜嗟嗣嗤嘉嘏嘤嘱嘴嘶嘹噀噎噤器噪噫噬嚎嚷嚼囊囚四回因团囧园困围囷固国图圃圆圈土圣在圭地场坂均坊坌坎坏坐坑块坚坛坞坟坠坡坤坦垂垒垓垕垛垠垢垣垦垫埃埋城域基堂堆堑堕堤堪堰堵塌塑塔塘塞填墀境墉墓墙增墟墨墩墵壁壎壑壕壤士壬壮声壳壶处备复夏夔夕外夙多夜够夤夥大天太夫夭央失头夷夸夹夺奁奂奄奇奈奉奋奎奏契奔奕奖套奚奠奢奥女奴奸好如妃妄妆妇妒妓妖妙妥妨妫妹妻妾姊始姐姑姓委姚姜姬姻姿威娄娇娘娥娩娱娴娶娼婆婉婚婢婴婿媒媚嫁嫂嫉嫌嫔嫡嫩嬉嬖嬴子孑孔孕字存孙孚孝孟季孤孥学孩孰孱孺孽宁宄宅宇守安宋完宏宓宕宗官宙定宛宜宝实宠审客宣室宥宦宪宫宰害宴宵家容宽宾宿寂寄寅密寇富寐寒寓寔寝寞察寡寤寨寮寰寸对寺寻导寿封射将尉尊小少尔尖尘尚尝尤尧尪就尸尹尺尼尽尾局层居屈屋屏屑展属屠屡履屦屯山岁岂岌岐岑岖岗岘岛岩岭岱岳岷岸峙峡峨峪峭峰峻崇崎崔崖崤崦崩嵋嵌嵩嵯嶲嶷巅巍川州巡巢工左巧巨巩巫差己已巳巴巷巽巾币市布帅帆师希帏帐帕帖帘帙帚帛帜帝带席帮帷常帻帼帽幄幅幔幕幡幢干平年并幸幻幼幽广庄庆庇床序庐庑库应底庖店庙庚府庞废度座庭庵庶康庸庾廉廊廒廖廙廛廨廪延廷建开异弃弄弇弈弊式弑弓引弗弘弛弟张弥弦弩弯弱弹强弼归当录彘彝形彤彦彧彩彪彬彭彰影彷役彻彼往征徂径待徇徊律徐徒得徘徙徜御徨循徭微徵德徼徽心忄必忆忌忍忒忖志忘忙忝忠忤忧快念忻忽忿怀态怅怆怎怏怒怕怖怜思怠急性怨怪怬怯总怿恁恂恃恋恍恐恒恕恙恢恣恤恨恩恪恬恭息恰恳恶恸恺恼恽悃悄悉悌悍悒悔悖悚悟悠患悦悬悯悲悴悼情惆惇惊惑惕惚惜惟惠惧惨惫惭惮惯惰想惶惹愀愁愆愈愉愍意愕愚感愠愤愧愬愿慄慈慌慎慑慓慕慢慧慨慰慵慷憎憔憩憾懈懊懋懑懒懔懦懿戆戈戊戌戍戎戏成我戒戕或战戚戟戢截戮戳戴户戾房所扁扇扈扉手才扎扑扒打托扛扣执扫扬扭扮扯扰扳扶批扼承抄把抑抒抓投抖抗折抚抛抟抠抡抢护报披抬抱抵抹押抽拂担拆拈拉拊拍拒拓拔拖拗拘拙拚招拜拟拣拥拦拨择括拭拯拱拳拴拷拼拽拾拿持挂指按挑挖挝挞挟挠挡挣挤挥挨挫振挺挽捆捉捍捎捏捐捕捞损换捧据捱捶捷捻捽掀授掉掌掎排掖掘掠探掣接控推掩措掬掳掷掾揎描提插揖握揣揪揭揲援揽搀搂搅搏搔搜搠搦搪搬搭搴携摄摆摇摔摘摧摩摸撄撇撑撒撚撞撤撩播撰撺撼擂擅操擎擐擒擞擢擦攀攒攘支收攸改攻放政故效敌敏救敕敖教敛敝敞敢散敦敬数敲整敷文斋斌斐斑斗料斛斜斟斡斤斥斧斩斫断斯新方於施旁旂旄旅旆旋旌族旒旗旙无既日旦旧旨早旬旰旱时旷旺旻昂昃昆昊昌明昏易昔昕星映春昧昨昭是昱昴昶昼显晃晋晌晏晒晓晔晙晚晤晦晨普景晴智暂暇暑暖暗暨暮暴暹曜曩曰曲更曷曹曼曾替最月有朋服朐朔朕朗望朝期朦木未末本札术朱朴朵机朽杀杂权杆杉李材村杖杜束杠条来杨杭杯杰杳杵松板极构枉析枒枕林枚果枝枢枣枪枫枭枯枰枳架枷枹柄柏某柑染柔柜查柩柯柱柳柴栅标栈栉栊栋栎栏树栖栗校株样核根格栽桀桂桃案桌桐桑桓桔桥桧桨桩桶梁梅梆梓梢梦梧梨梯械梳检棉棋棍棒棘棚森棹棺椁椅植椒椽楙楚楞楫楮楯楷楼概榆榇榛榜榻槁槊槎槛槽樊樗模横樯樱樵樽橹檀檄欠次欢欣欤欲欷欺款歃歆歇歉歌歔止正此步武歹死歼殁殂殃殄殆殉殊残殒殓殖殚殛殡殪殴段殷殿毁毅毋母每毒毓比毕毖毗毙毛毡毫氅氏民气氛水永汁求汇汉汗汙汜汝江池污汤汪汰汲汶汹沂沃沅沆沈沉沌沐沓沔沙沛沟没沥沦沧沪沮沱河沸油治沽沾沿泄泉泊泌法泗泛泞泠波泣泥注泪泯泰泸泼泽泾洁洋洒洗洛洞津洪洮洱洲洹活洽派流浅浆浇浊测济浑浓浔浙浚浦浩浪浮浴海浸浼涂消涉涌涎涓涕涛涝涟涣润涧涨涪液涿淄淆淇淋淑淘淝淡淫淮淯深淳混淹添清渊渎渐渑渔渗渚渝渠渡渤温渭港渴游渺湄湖湘湫湿溃溅源溜溟溢溪溯溺溽滂滋滏滑滔滕滚滞满滥滨滩滴漆漏漓演漠漫漯漳潘潜潢潭潮潸潺潼澄澜激濛濡濦濬濮濯瀣灌火灭灯灰灵灶灸灼灾灿炉炎炜炫炬炭炮炳点烂烈烘烛烝烟烦烧热烹烽焉焕焙焚焦焫焰然煌煎照煨煮煽熊熙熟熬燃燎燕燥爨爪爬爰爱爵父爷爻爽牁牂片版牌牒牙牛牝牟牡牢牧物牲牵特牺犀犊犍犒犬犭犯状犹狂狄狈狎狐狗狝狠狡狩独狭狮狱狼猇猊猎猖猛猜猝猥猪献猾猿獐獗獠獬獭玄率玈玉王玠玩环现玲玷玺玻珍珑珝珠珩珪班球琅理琊琐琢琦琪琬琮琰琳琴琼瑁瑕瑚瑛瑜瑞瑟瑯瑶瑾璃璆璇璋璜璝璧璩璿瓒瓘瓜瓦瓮瓯瓶甄甑甘甚甜生甥用甫甬田由甲申电男甸画畅畋界畏畔留畜略番畯畴畿疆疏疑疗疠疢疥疫疮疲疴疼疽疾病症痈痊痒痕痛痢痴瘁瘠瘤瘦瘴癖癞癣癸登白百皂的皆皇皈皋皎皓皖皮皿盂盆盈益盍盏盐监盒盔盖盗盘盛盟盩目盱盲直相盼盾省眇眉看眙真眠眦眩眭眷眸眺眼着睁睚睛睡督睦睨睬睹睿瞋瞑瞒瞪瞰瞳瞻矍矛矜矢矣知矩矫短石矴矿砀砂砌砍砖砚破砾硕硝硫硬确碌碍碎碑碗碣碧碱碾磋磐磨磾示礼社祀祁祈祎祐祖祚祜祝神祠祢祥票祭祯祷祸祺禀禁禄禅福禧禳禹离禽禾秀私秃秉秋种科秘租秣秦秩秬秭积称移秽稀程稍税稔稚稠稳稷稻稼稽稿穆穰穴究穷穹空穿突窃窄窍窗窘窜窝窟窠窥窦立竖站竞竟章竣童竭端竹竺竿笃笄笑笔笙笛笠符第笮笳笺笼等筋筏筑答策筛筮筵筹简箕算管箧箪箭箱箸篁篆篇篙篡篱篷簇簏簧簪簿籍米类粉粒粗粝粟粥粪粮粱粲粹精糊糜糟系紞素索紧紫累絏絮綝繁繇纂纛纠纡红纣约级纪纬纭纮纯纱纲纳纵纶纷纸纹纽线绁练绅细织终绊绍绎经绑绒结绕绘给绛络绝绞统绢绣绥绦继绩绪绫续绮绰绲绳维绵绶综绽绿缀缄缆缉缎缓缔缕编缘缙缚缝缟缠缢缣缧缨缩缭缮缰缴缵缺罄罐网罔罕罗罚罡罢罩罪置署罹罾羁羊羌美羕羞羡群羲羸羹羽羿翁翅翊翎翔翘翟翠翩翰翱翳翻翼耀老考者耆而耐耑耒耕耗耳耶耸耻耽耿聊聋职联聘聚聪肃肆肉肋肌肓肖肘肚肝肠股肢肤肥肩肯肱育肴肺肿胀胁胃胄胆背胎胖胜胞胡胤胥胧胪胯胶胸能脂脉脊脍脏脐脑脚脯脱脸腊腐腑腔腕腥腮腰腴腹腾腿膀膂膊膏膑膛膝膳膺臂臣臧自臭至致臻臼臾舄舅舆舌舍舒舜舞舟般舰舱舵舸船艘艟艨良艰色艳艺艾节芒芜芝芟芥芦芬花芳芸芽苇苌苍苏苑苒苗苛苞苟若苦英苴苹茂范茅茔茕茨茫茬茵茶荀荆草荏荐荒荡荣荥荧荩荫药荷荻荼莅莒莘莞莩莫莱莲获莹莽菜菲菽萁萃萌萤营萧落葆著葛董葫葬葭葺蒂蒋蒙蒜蒯蒲蒸蒹蒺蒿蓄蓍蓝蓦蓬蔑蔓蔚蔡蔬蔺蔽蕃蕤蕲蕴薄薛薤薨薪薰藁藉藏藐藜藤藩蘸蘼虎虏虐虑虔虚虞虢虫虬虹虽虾虿蚁蚕蛇蛉蛙蛛蛟蛮蛱蜀蜂蜈蜉蜘蜜蜺蜾蝇蝉蝎蝗蝣蝶蝼螂融螟螳螺蟊蟠蠡蠢蠫蠹血衄衅行衍衔街衙衠衡衢衣补表衫衬衮衰衷衽衿袁袄袅袋袍袒袖袛袤被袭裁裂装裒裔裘裙裤裨裴裸裹裾褐褒褚褥襄襟西要覆覈见观规觅视览觉觐觑角觜觞解觥触觫觳言誉誓譔警譬计订讣认讥讨让讪讫训议讯记讲讳讴讶许讹论讼讽设访诀证评识诈诉诊诌词诏试诗诘诚诛话诞诠诡询诣诤该详诩诫诬语误诰诱诲诳说诵请诸诹诺读课谀谁调谄谅谈谊谋谌谍谏谐谑谒谓谕谗谘谙谚谛谞谡谢谣谤谥谦谧谨谪谬谭谮谯谱谲谴谶谷豁豆豕豚象豨豪豫豸豹豺貂貅貌貔賨贝贞负贡财责贤败货质贩贪贫贬购贮贯贰贱贲贴贵费贺贻贼贽贾贿赀赂赃资赈赋赌赍赎赏赐赖赘赚赛赞赠赡赢赤赦赧赫赭走赴赵赶起趁超越趋趱足趷跃跄跋跌跎跑跖跛距跞跟跣跨跪路跳践跶跸跼踅踉踊踌踏踞踢踪踵蹄蹇蹈蹉蹊蹋蹐蹑蹙蹬蹶躁躄躇躔身躬躯躲轘车轨轩转轮软轰轲轴轵轸轻载轿辂较辄辅辆辇辈辉辍辎辑输辔辕辖辛辜辞辟辨辩辰辱边辽达迁迂迄迅过迈迎运近迓返还这进远违连迟迢迤迩迫迭述迷迸迹追退送适逃逅逆选逊逍透逐递途逗通逝逞速造逡逢逦逮逵逶逸逻逼逾遁遂遇遍遏遐遑道遗遣遥遨遭遮遵遽避邀邂邈邑邓邕邙邢那邦邪邮邯邰邱邳邴邵邸邹邺邻邽邾郁郃郊郎郏郑郗郝郡郢郤郦部郭郯郸都郿鄂鄄鄙鄠鄢鄣鄱酂酉酋酌配酎酒酗酣酥酬酱酷酸酹酾酿醉醒醢醮醴采释里重野量金釜釭釿鈇鈚鉴銮鍪鏖钁针钉钓钗钝钟钢钦钧钩钱钳钵钺铁铃铜铠铨铭铮银铸铺链销锁锄锅锋锐错锡锢锣锤锥锦锭键锯锵锹镇镌镔镜镝镫镬镰镶长闇门闪闭问闲间闵闷闸闹闺闻闼闾闿阁阃阄阅阆阉阊阍阎阐阑阔阖阙阚阜队阡阪阱防阳阴阵阶阻阿陂附际陆陇陈陋陌降限陕陛陟院除陨险陪陵陶陷隅隆隈随隐隔隗隘隙障隧隰隳隶隽难雀雁雄雅集雉雌雍雏雒雕雝雠雨雪雳零雷雹雾需霁霄霆震霍霎霏霖霜霞露霸霹青靓靖静非靠靡面革靬靳靴靶鞅鞍鞒鞘鞠鞦鞭韦韧韩韪韬音韵韶顒顗顶顷项顺须顽顾顿颀颂预颅领颇颈颊颌颍颐频颓颔颖颗题颜额颠颤风飏飐飘飙飞食飨餐餮饕饥饬饭饮饯饰饱饵饶饷饼饿馀馁馆馈馑馒馔首馗馘香馥馨騕马驭驮驯驰驱驴驷驸驹驻驼驽驾驿骁骂骄骅骆骇骈骋验骏骑骖骘骚骝骞骠骡骤骥骧骨骸髀髓高髡髦髫髭髯髻鬃鬅鬓鬯鬻鬼魁魂魄魅魇魏鱼鲁鲂鲈鲍鲜鲧鲲鲵鲸鳅鳌鳖鳝鳞鸟鸠鸡鸣鸦鸩鸭鸯鸳鸷鸾鸿鹄鹅鹉鹊鹏鹗鹤鹦鹩鹪鹭鹰鹿麈麋麒麟麦麴麻麾黄黍黎黑默黛黜黥黩黯鼎鼐鼓鼕鼙鼠鼻齁齐齑齿龄龙龚龛龟！（），：；？\n",
      "不同字符数量: 3,951\n"
     ]
    }
   ],
   "source": [
    "# 计算《三国演义》中有多少个不同的字符\n",
    "\n",
    "# set去重 --> list转换成列表 --> 排序\n",
    "chars = sorted(list(set(data)))\n",
    "# 不同字符的数量\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"所有不同的字符:\", ''.join(chars))\n",
    "print(f\"不同字符数量: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82a5dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3934\n"
     ]
    }
   ],
   "source": [
    "# 创建从字符到整数的映射\n",
    "\n",
    "# 从字符到整数的映射字典\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# 从整数到字符的映射字典\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# 例如我们可以看一下`鼻`这个字对应的整数\n",
    "print(stoi['鼻'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9c3adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2044, 2044, 3600, 1881, 40, 3429, 1871]\n",
      "潼潼阊没之遣沆\n"
     ]
    }
   ],
   "source": [
    "# 给定一个字符串`s`, 输入字符串中每个字对应的整数组成的列表\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "# 给定一个整数列表, 返回列表中每个整数对应的字符所组成的字符串\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# 测试一下\n",
    "print(encode('滚滚长江东逝水'))\n",
    "print(decode([2066, 2066, 3623, 1903, 62, 3452, 1893]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "038919b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据集\n",
    "# 将《三国演义》前90%的文字作为训练数据集\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "# 将《三国演义》后10%的文字作为验证数据集\n",
    "val_data = data[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35ba559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集中有 544,993 个字符(token)\n",
      "验证数据集中有 60,555 个字符(token)\n"
     ]
    }
   ],
   "source": [
    "# 分别将训练数据集中的字符和验证数据集中的字符编码成整数\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "\n",
    "print(f\"训练数据集中有 {len(train_ids):,} 个字符(token)\")\n",
    "print(f\"验证数据集中有 {len(val_ids):,} 个字符(token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7183a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练数据集和验证数据集分别保存成二进制文件\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile('train.bin')\n",
    "val_ids.tofile('val.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1163a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将元数据保存成pickle格式的文件, 供我们后面在encode或者decode时使用\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open('meta.pkl', 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955c76a",
   "metadata": {},
   "source": [
    "我们数据准备的工作就完成了.\n",
    "\n",
    "# 编写GPT模型\n",
    "\n",
    "接下来我们开始编写模型代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "014fe790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入需要的一些包\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d41863",
   "metadata": {},
   "source": [
    "## GeLU激活函数\n",
    "\n",
    "公式如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66d53c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义GELU激活函数, 具体论文参见:\n",
    "# https://arxiv.org/abs/1606.08415\n",
    "def new_gelu(x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9cca6",
   "metadata": {},
   "source": [
    "## 层归一化模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e6dea4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义层归一化模块\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5037a",
   "metadata": {},
   "source": [
    "## 因果自注意力机制模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e1c6f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 确保词嵌入向量的维度是head数量的整数倍\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # 下面的线性变换计算的是:\n",
    "        # 在一批中, 所有头的key, query, value的投影\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # 下面的线性变换的作用是将投影输出\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # head的数量\n",
    "        self.n_head = config.n_head\n",
    "        # 词嵌入向量的维度\n",
    "        self.n_embd = config.n_embd\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        定义因果自注意力模块在接收到张量x时, 输出什么样的张量\n",
    "        \"\"\"\n",
    "        B, T, C = x.size() # 批的大小(batch size), 序列长度(sequence length), 词嵌入向量维度(n_embd)\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        \n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)\n",
    "        \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        \n",
    "        return self.c_proj(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03458c",
   "metadata": {},
   "source": [
    "## 多层感知机模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d2fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = new_gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce4f64",
   "metadata": {},
   "source": [
    "## Block模块\n",
    "\n",
    "![](assets/Block模块示意图.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f4090c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b21eb6",
   "metadata": {},
   "source": [
    "## GPT模型的一些参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3bc819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 64\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 4\n",
    "    n_embd: int = 128\n",
    "    bias: bool = False # 不使用偏置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881e885",
   "metadata": {},
   "source": [
    "## GPT模型的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e5af9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        # Transformer模块\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # 初始化所有权重\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # 打印模型的参数数量\n",
    "        print(\"参数数量: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "        \n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.2)\n",
    "                \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"无法前馈(向前发送)序列长度: {t}, 因为block size只有{self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "        \n",
    "        tok_emb = self.transformer.wte(idx) # token嵌入向量的形状 (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # 位置嵌入向量的形状 (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # subtle: 'transformer.wte.weight' and 'lm_head.weight' are tied, so they\n",
    "        # will appear in the no_decay and decay sets respectively after the above.\n",
    "        # In addition, because named_parameters() doesn't return duplicates, it\n",
    "        # will only return the first occurence, key'd by 'transformer.wte.weight', below.\n",
    "        # so let's manually remove 'lm_head.weight' from decay set. This will include\n",
    "        # this tensor into optimization via transformer.wte.weight only, and not decayed.\n",
    "        decay.remove('lm_head.weight')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f7903",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "\n",
    "由于配置问题, 我们使用CPU来训练GPT模型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4556b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 选择一个随机种子\n",
    "torch.manual_seed(1337)\n",
    "# 设备类型\n",
    "device = 'cpu'\n",
    "block_size = 64\n",
    "batch_size = 12\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "# 初始化一个空的上下文\n",
    "ctx = nullcontext()\n",
    "\n",
    "# 加载训练数据集和验证数据集\n",
    "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5967f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义切分数据集为输入和标签的函数\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5408cbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 3951 (inside 'meta.pkl')\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0 # 迭代次数\n",
    "best_val_loss = 1e9 # 最佳损失\n",
    "\n",
    "with open('meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "meta_vocab_size = meta['vocab_size']\n",
    "print(f\"found vocab_size = {meta_vocab_size} (inside 'meta.pkl')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "536e9502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从头开始训练模型\n",
      "GPTConfig(block_size=64, vocab_size=3951, n_layer=4, n_head=4, n_embd=128, bias=False)\n",
      "参数数量: 1.29M\n",
      "using fused AdamW: False\n",
      "可不惧乎？愿君侯裒多益寡，非礼勿履：然后三公可至，青蝇可驱也。”邓飏怒曰：“此老生之常谈耳！”辂曰：“老生者见不生，常谈者见不谈\n",
      "不惧乎？愿君侯裒多益寡，非礼勿履：然后三公可至，青蝇可驱也。”邓飏怒曰：“此老生之常谈耳！”辂曰：“老生者见不生，常谈者见不谈。\n"
     ]
    }
   ],
   "source": [
    "# 从头开始训练模型\n",
    "print(\"从头开始训练模型\")\n",
    "gptconf = GPTConfig()\n",
    "gptconf.vocab_size = meta_vocab_size\n",
    "print(gptconf)\n",
    "model = GPT(gptconf)\n",
    "model.to(device)\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
    "\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "print(decode(X[1].tolist()))\n",
    "print(decode(Y[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "93556b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估算损失\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0419aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "\n",
    "# 训练代码\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"将模型的检查点文件保存到 {out_dir}\")\n",
    "                torch.save(checkpoint, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        lossf = loss.item() # loss as float. note: this is a CPU-GPU sync point\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
